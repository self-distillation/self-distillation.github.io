<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>SDPO: Reinforcement Learning via Self-Distillation</title><link rel="stylesheet" href="common.css"/><style>
    /* SDPO-specific overrides */
    @media only screen and (max-width: 768px) {
        body.paper.sdpo .sdpo-model-scale-figure {
            margin-top: 0 !important;
        }
    }

    /* SDPO-specific mobile tweaks for model-scale section */
    @media (max-width: 768px) {
        body.paper.sdpo .sdpo-model-scale {
            justify-content: flex-start;
            gap: 0.4em;
        }

        body.paper.sdpo .sdpo-model-scale .column {
            margin-bottom: 0;
        }

        body.paper.sdpo .sdpo-model-scale figure.image {
            margin-bottom: 0.2em;
        }
    }
    </style></head><body class="paper sdpo"><article class="page sans"><header><h1 class="page-title">Reinforcement Learning via Self-Distillation</h1><p class="page-description"></p></header><div class="page-body"><p><a href="https://jonhue.github.io/">Jonas Hübotter</a><sup>1</sup>,  <a href="https://frederike-luebeck.de/">Frederike Lübeck</a><sup>*,1,2</sup>,  <a href="https://www.linkedin.com/in/lbehric">Lejs Behric</a><sup>*,1</sup>,  <a href="https://antonbaumann.com/">Anton Baumann</a><sup>*,1</sup>,  <a href="https://marbaga.github.io/">Marco Bagatella</a><sup>1,2</sup>,  <a href="https://daniellsm.github.io/">Daniel Marta</a><sup>1</sup>,  <a href="https://idoh.github.io/">Ido Hakimi</a><sup>1</sup>,  <a href="https://idanshen.github.io/">Idan Shenfeld</a><sup>3</sup>,  <a href="https://thomasklbg.github.io/">Thomas Kleine Buening</a><sup>1</sup>,  <a href="https://guestrin.su.domains/">Carlos Guestrin</a><sup>4</sup>,  <a href="https://las.inf.ethz.ch/krausea">Andreas Krause</a><sup>1</sup></p><p><sup>1</sup>ETH Zurich <sup>2</sup>Max Planck Institute for Intelligent Systems <sup>3</sup>MIT <sup>4</sup>Stanford</p><p><a href="https://arxiv.org/abs/2601.20802"><strong>Paper</strong></a>,  <a href="https://github.com/lasgroup/SDPO"><strong>Code</strong></a>,  <a href="https://wandb.ai/jonhue/SDPO?nw=mgotcx6kk7"><strong>W&B Logs</strong></a></p><p>
    </p><figure class="image" style="text-align:center;margin-bottom:3.5em;"><a href="figures/SDPO/hero_small.png"><img style="width:100%;max-width:800px;" src="figures/SDPO/hero_small.png"/></a></figure><h3><strong>Abstract </strong></h3><p>Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and <mark class="highlight-blue"><strong>introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model.</strong></mark> SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.</mark></em></p><figure class="image"><center><a href="figures/SDPO/sdpo.png"><img style="width:100%;max-width:800px;" src="figures/SDPO/sdpo.png"/></a><figcaption style="width:100%;max-width:800px;font-size:0.9rem;color:#444;margin-top:0em;"><strong>SDPO</strong>: sample a rollout, obtain tokenized feedback, re-evaluate the rollout under a feedback-conditioned self-teacher, and distill the teacher's corrected next-token distribution into the policy.</figcaption></center></figure><p>
    </p><h3><strong>Learning to Reason Better, Faster</strong></h3><figure class="image" style="text-align:left;"><a href="figures/SDPO/10xrun.png"><img style="width:100%;max-width:700px;display:block;margin-left:0;margin-right:auto;" src="figures/SDPO/10xrun.png"/></a></figure><p>Even when the environment provides only a scalar outcome reward, SDPO outperforms GRPO substantially. SDPO uses successful rollouts as implicit feedback, allowing the policy to directly learn from its own best generations, without requiring external demonstrations or additional datasets. On undergrad-level Chemistry questions (shown above with Olmo3-7B-Instruct), <mark class="highlight-blue"><strong>SDPO reaches GRPO's accuracy 10× faster (wall-clock time) and reaches a much higher final accuracy.</strong></mark> This shows that self-distillation can remove the credit-assignment bottleneck of today's RLVR methods. Reasoning traces after SDPO are up to 7× shorter than those of GRPO which often enters logical loops, highlighting that effective reasoning need not be verbose.</p><p>
    </p><h3><strong>Test-Time Self-Distillation Accelerates Discovery</strong></h3><figure class="image" style="text-align:left;"><a href="figures/SDPO/ttt_results.png"><img style="width:100%;max-width:700px;display:block;margin-left:0;margin-right:auto;" src="figures/SDPO/ttt_results.png"/></a></figure><p>SDPO can also be applied at test time to a single hard question by repeatedly distilling the model with environment feedback into the current policy. With binary rewards, RLVR provides no learning signal before the first success, so it cannot accelerate discovery in the same way. <mark class="highlight-blue"><strong>SDPO accelerates the discovery of solutions to very hard problems of LiveCodeBench (pass@64 < 0.03).</strong></mark> It achieves the same discovery probability (discovery@k = P(solution found within k attempts)) as best-of-k sampling and multi-turn prompting with up to ~3× fewer attempts, and discovers solutions the baselines fail to find at all. Rich environment feedback (such as runtime errors) turns repeated failures into a dense learning signal, enabling SDPO to progress even before the first solution is found.</p><p>
    </p><hr style="margin-bottom: 4em;"/><h3><strong>SDPO Benefits from Model Scale</strong></h3><div class="column-list sdpo-model-scale"><div style="width:43.75%" class="column"><p><mark class="highlight-blue"><strong>SDPO's gains grow with model size.</strong></mark> Across Qwen3 models (0.6B→8B), SDPO consistently outperforms GRPO, with the gap widening as models become stronger. This suggests that accurate retrospection of the self-teacher emerges with scale as models become better in-context learners, making self-distillation increasingly effective for larger LLMs.</p><p>
    </p></div><div style="width:56.25%" class="column"><figure class="image sdpo-model-scale-figure" style="text-align:left;margin-top:-2em;"><a href="figures/SDPO/model_scaling.png"><img style="width:2370px;display:block;margin-left:0;margin-right:auto;" src="figures/SDPO/model_scaling.png"/></a></figure>
    <!-- </p></div></div><h3><strong>TBD</strong></h3><div class="column-list"><div style="width:43.75%" class="column"><p>...</p><p>
    </p></div><div style="width:56.25%" class="column"><figure class="image" style="margin-top:0;"><a href="figures/SDPO/credit_assignment.png"><img style="width:2370px" src="figures/SDPO/credit_assignment.png"/></a></figure><p> -->
    </p></div></div><h3><strong>SDPO Performs Dense Credit Assignment</strong></h3><figure class="image"><a href="figures/SDPO/advantages.png"><center><img style="width:3254px" src="figures/SDPO/advantages.png"/></a><figcaption style="width:100%;max-width:800px;font-size:0.9rem;color:#444;margin-top:0em;"><strong>Left</strong>: Advantages of SDPO vs GRPO in the above example (red: negative advantage, blue: positive advantage).</br><strong>Right</strong>: Advantages of SDPO vs GRPO during training (row: sampled rollout, position: advantage at token position).</figcaption></center></figure><p>SDPO provides dense credit assignment by comparing the student's next-token distribution to the self-teacher's distribution with in-context feedback. SDPO assigns positive/negative advantages when the self-teacher becomes more/less confident than the student, providing dense supervision along the student's attempts. This targeted signal is consistent with both SDPO's faster learning and its convergence to more efficient reasoning.</p>
    <hr style="margin-bottom: 4em;"/><h3><strong>Bibtex</strong></h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre class="code code-wrap"><code class="language-Plain Text" style="white-space:pre;word-break:normal">@article{hubotter2026reinforcement,
    title = {Reinforcement Learning via Self-Distillation},
    author = {Hübotter, Jonas and Lübeck, Frederike and Behric, Lejs and Baumann, Anton and Bagatella, Marco and Marta, Daniel and Hakimi, Ido and Shenfeld, Idan and Kleine Buening, Thomas and Guestrin, Carlos and Krause, Andreas},
    year = {2026},
    journal = {arXiv preprint arXiv:2601.20802},
}</code></pre><p>
    </p></div></article><div class="page-body sans"><p class="nav-link-wrap"><span class="nav-links"><a class="button-link-plain" href="index.html"><span class="button-label">Home</span></a><a class="button-link" href="SDFT.html"><span class="button-label">Self-Distillation Enables Continual Learning (SDFT)</span></a></span></p></div><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>
