<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Aligning Language Models from User Interactions</title><link rel="stylesheet" href="common.css"/><style>
    /* SDPO-specific overrides */
    @media only screen and (max-width: 768px) {
        body.paper.sdpo .sdpo-model-scale-figure {
            margin-top: 0 !important;
        }
    }

    /* SDPO-specific mobile tweaks for model-scale section */
    @media (max-width: 768px) {
        body.paper.sdpo .sdpo-model-scale {
            justify-content: flex-start;
            gap: 0.4em;
        }

        body.paper.sdpo .sdpo-model-scale .column {
            margin-bottom: 0;
        }

        body.paper.sdpo .sdpo-model-scale figure.image {
            margin-bottom: 0.2em;
        }
    }
    </style></head><body class="paper sdpo"><article class="page sans"><header><h1 class="page-title">Aligning Language Models from User Interactions</h1><p class="page-description"></p></header><div class="page-body"><p><a href="https://thomasklbg.github.io/">Thomas Kleine Buening</a><sup>1</sup>,  <a href="https://jonhue.github.io/">Jonas Hübotter</a><sup>1</sup>,  <a href="https://www.linkedin.com/in/barna-pasztor">Barna Pásztor</a><sup>1</sup>,  <a href="https://idanshen.github.io/">Idan Shenfeld</a><sup>2</sup>,  <a href="https://sites.google.com/view/alpi-lab/giorgia-ramponi">Giorgia Ramponi</a><sup>3</sup>,  <a href="https://las.inf.ethz.ch/krausea">Andreas Krause</a><sup>1</sup></p><p><sup>1</sup>ETH Zurich <sup>2</sup>MIT <sup>3</sup>University of Zurich</p><p><strong>Paper: </strong><a href="https://arxiv.org/abs/XXX">https://arxiv.org/abs/XXX</a></p><p><strong>Code: </strong><a href="https://github.com/XXX">https://github.com/XXX</a></p><p>
    </p><figure class="image"><center><a href="figures/user_interactions/diagram.png"><img style="width:100%;max-width:1000px;" src="figures/user_interactions/diagram.png"/></a><figcaption style="width:100%;max-width:800px;font-size:0.9rem;color:#444;margin-top:0em;"><strong>Learning from user interactions with SDPO</strong>: sample an assistant turn, wait for the next user turn, then recompute assistant's logits in hindsight with user turn in context, and distill this corrected next-token distribution into the policy.</figcaption></center></figure><h3><strong>Abstract </strong></h3><p>Multi-turn user interactions are among the most abundant data produced by language models, yet we lack effective methods to learn from them. While typically discarded, these interactions often contain useful information: follow-up user messages may indicate that a response was incorrect, failed to follow an instruction, or did not align with the user’s preferences. Importantly, language models are already able to make use of this information in context. After observing a user’s follow-up, the same model is often able to revise its behavior. <mark class="highlight-blue"><strong>We leverage this ability to propose a principled and scalable method for learning directly from user interactions through self-distillation.</strong></mark> By conditioning the model on the user's follow-up message and comparing the resulting token distribution with the original policy, we obtain a target for updating the policy that captures how the model's behavior changes in hindsight. We then distill this hindsight distribution back into the current policy. Remarkably, we show that training on real-world user conversations from WildChat improves language models across standard alignment and instruction-following benchmarks, without regressing other capabilities. The same mechanism enables personalization, allowing models to continually adapt to individual users through interaction without explicit feedback. Our results demonstrate that raw user interactions arising naturally during deployment enable alignment, personalization, and continual adaptation.</mark></em></p><p>
    </p><h3><strong>Test-Time Self-Distillation Accelerates Discovery</strong></h3><figure class="image" style="text-align:left;"><a href="figures/user_interactions/benchmarks.png"><img style="width:100%;max-width:700px;display:block;margin-left:0;margin-right:auto;" src="figures/user_interactions/benchmarks.png"/></a><figcaption style="width:100%;max-width:800px;font-size:0.9rem;color:#444;margin-top:0em;">Training on 14k real-world user conversations from WildChat improves performance across diverse benchmarks.</figcaption></figure><p>SDPO can also be applied at test time to a single hard question by repeatedly distilling the model with environment feedback into the current policy. With binary rewards, RLVR provides no learning signal before the first success, so it cannot accelerate discovery in the same way. <mark class="highlight-blue"><strong>SDPO accelerates the discovery of solutions to very hard problems of LiveCodeBench (pass@64 < 0.03).</strong></mark> It achieves the same discovery probability (discovery@k = P(solution found within k attempts)) as best-of-k sampling and multi-turn prompting with up to ~3× fewer attempts, and discovers solutions the baselines fail to find at all. Rich environment feedback (such as runtime errors) turns repeated failures into a dense learning signal, enabling SDPO to progress even before the first solution is found.</p><p>
    </p><h3><strong>Learning to Reason Better, Faster</strong></h3><figure class="image" style="text-align:left;"><a href="figures/user_interactions/personalization.png"><img style="width:100%;max-width:700px;display:block;margin-left:0;margin-right:auto;" src="figures/user_interactions/personalization.png"/></a><figcaption style="width:100%;max-width:800px;font-size:0.9rem;color:#444;margin-top:0em;">SDPO enables continual personalization without catastrophic forgetting when sequentially exposed to complementary user preferences.</figcaption></figure><p>Even when the environment provides only a scalar outcome reward, SDPO outperforms GRPO substantially. SDPO uses successful rollouts as implicit feedback, allowing the policy to directly learn from its own best generations, without requiring external demonstrations or additional datasets. On undergrad-level Chemistry questions (shown above with Olmo3-7B-Instruct), <mark class="highlight-blue"><strong>SDPO reaches GRPO's accuracy 6× faster (wall-clock time) and reaches a much higher final accuracy.</strong></mark> This shows that self-distillation can remove the credit-assignment bottleneck of today's RLVR methods. Reasoning traces after SDPO are up to 11× shorter than those of GRPO which often enters logical loops, highlighting that effective reasoning need not be verbose.</p><p>
    </p><strong><h3>SDPO Benefits from Model Scale</strong></h3><div class="column-list sdpo-model-scale"><div style="width:43.75%" class="column"><p><mark class="highlight-blue"><strong>SDPO's gains grow with model size.</strong></mark> Across Qwen3 models (0.6B→8B), SDPO consistently outperforms GRPO, with the gap widening as models become stronger. This suggests that accurate retrospection of the self-teacher emerges with scale as models become better in-context learners, making self-distillation increasingly effective for larger LLMs.</p><p>
    </p></div><div style="width:56.25%;white-space:initial" class="column"><figure class="image sdpo-model-scale-figure" style="text-align:left;margin-top:-2em;"><a href="figures/user_interactions/advantages.png"><img style="width:2370px;display:block;margin-left:0;margin-right:auto;" src="figures/user_interactions/advantages.png"/></a></figure>
    <!-- </p></div></div><h3><strong>TBD</strong></h3><div class="column-list"><div style="width:43.75%" class="column"><p>...</p><p>
    </p></div><div style="width:56.25%" class="column"><figure class="image" style="margin-top:0;"><a href="figures/SDPO/credit_assignment.png"><img style="width:2370px" src="figures/SDPO/credit_assignment.png"/></a></figure><p> -->
    </p></div></div>
    <h3><strong>Bibtex</strong></h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre class="code code-wrap"><code class="language-Plain Text" style="white-space:pre;word-break:normal">@article{buening2026aligning,
    title = {Aligning Language Models from User Interactions},
    author = {Kleine Buening, Thomas and Hübotter, Jonas and Pásztor, Barna and Shenfeld, Idan and Ramponi, Giorgia and Krause, Andreas},
    year = {2026},
    journal = {arXiv preprint arXiv:XXX},
}</code></pre><p>
    </p></div></article><div class="page-body sans"><p class="nav-link-wrap"><span class="nav-links"><a class="button-link-plain" href="index.html"><span class="button-label">Self-Distillation</span></a><a class="button-link" href="SDFT.html"><span class="button-label">Self-Distillation Enables Continual Learning (SDFT)</span></a></span></p></div><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>
