<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Aligning Language Models from User Interactions</title><link rel="stylesheet" href="common.css"/><style>
    /* SDPO-specific overrides */
    @media only screen and (max-width: 768px) {
        body.paper.sdpo .sdpo-model-scale-figure {
            margin-top: 0 !important;
        }
    }

    /* SDPO-specific mobile tweaks for model-scale section */
    @media (max-width: 768px) {
        body.paper.sdpo .sdpo-model-scale {
            justify-content: flex-start;
            gap: 0.4em;
        }

        body.paper.sdpo .sdpo-model-scale .column {
            margin-bottom: 0;
        }

        body.paper.sdpo .sdpo-model-scale figure.image {
            margin-bottom: 0.2em;
        }
    }
    </style></head><body class="paper sdpo"><article class="page sans"><header><h1 class="page-title">Aligning Language Models from User Interactions</h1><p class="page-description"></p></header><div class="page-body"><p><a href="https://thomasklbg.github.io/">Thomas Kleine Buening</a><sup>1</sup>,  <a href="https://jonhue.github.io/">Jonas Hübotter</a><sup>1</sup>,  <a href="https://www.linkedin.com/in/barna-pasztor">Barna Pásztor</a><sup>1</sup>,  <a href="https://idanshen.github.io/">Idan Shenfeld</a><sup>2</sup>,  <a href="https://sites.google.com/view/alpi-lab/giorgia-ramponi">Giorgia Ramponi</a><sup>3</sup>,  <a href="https://las.inf.ethz.ch/krausea">Andreas Krause</a><sup>1</sup></p><p><sup>1</sup>ETH Zurich <sup>2</sup>MIT <sup>3</sup>University of Zurich</p><p><a href="user_interactions.pdf"><strong>Paper</strong></a>,  <a href="https://github.com/lasgroup/user_interactions"><strong>Code</strong></a></p><p>
    </p><figure class="image"><center><a href="figures/user_interactions/diagram.png"><img style="width:100%;max-width:1000px;" src="figures/user_interactions/diagram.png"/></a><figcaption style="width:100%;max-width:800px;font-size:0.9rem;color:#444;margin-top:0em;"><strong>Learning from user interactions with SDPO</strong>: sample an assistant turn, wait for the next user turn, then recompute assistant's logits in hindsight with user turn in context, and distill this corrected next-token distribution into the policy.</figcaption></center></figure><h3><strong>Abstract </strong></h3><p>Multi-turn user interactions are among the most abundant data produced by language models, yet we lack effective methods to learn from them. While typically discarded, these interactions often contain useful information: follow-up user messages may indicate that a response was incorrect, failed to follow an instruction, or did not align with the user’s preferences. Importantly, language models are already able to make use of this information in context. After observing a user’s follow-up, the same model is often able to revise its behavior. <mark class="highlight-blue"><strong>We leverage this ability to propose a principled and scalable method for learning directly from user interactions through self-distillation.</strong></mark> By conditioning the model on the user's follow-up message and comparing the resulting token distribution with the original policy, we obtain a target for updating the policy that captures how the model's behavior changes in hindsight. We then distill this hindsight distribution back into the current policy. Remarkably, we show that training on real-world user conversations from WildChat improves language models across standard alignment and instruction-following benchmarks, without regressing other capabilities. The same mechanism enables personalization, allowing models to continually adapt to individual users through interaction without explicit feedback. Our results demonstrate that raw user interactions arising naturally during deployment enable alignment, personalization, and continual adaptation.</mark></em></p><p>
    </p><h3><strong>Improving LLMs by Learning from Raw User Interactions</strong></h3><figure class="image" style="text-align:left;"><a href="figures/user_interactions/benchmarks.png"><img style="width:100%;max-width:800px;display:block;margin-left:0;margin-right:auto;" src="figures/user_interactions/benchmarks.png"/></a><figcaption style="width:100%;max-width:1000px;font-size:0.9rem;color:#444;margin-top:0em;">Training on 14k real-world user conversations from WildChat improves performance across diverse benchmarks.</figcaption></figure><p><mark class="highlight-blue"><strong>Training with SDPO on raw, real-world user conversations consistently improves alignment and instruction-following, reasoning, and creative writing.</strong></mark> In SDPO, given a user follow-up to the model's response, the model decides how it would change its response in hindsight. This achieves gains across multiple benchmarks without using explicit rewards, preference labels, or curated supervision. Meanwhile, learning from real-world conversations does not regress any other capabilities. We can use real-world conversations to improve language models!</p><p>
    </p><h3><strong>Enabling Continual Personalization to User Preferences</strong></h3><figure class="image" style="text-align:left;"><a href="figures/user_interactions/personalization.png"><img style="width:100%;max-width:750px;display:block;margin-left:0;margin-right:auto;" src="figures/user_interactions/personalization.png"/></a><figcaption style="width:100%;max-width:1000px;font-size:0.9rem;color:#444;margin-top:0em;">SDPO enables continual personalization without catastrophic forgetting when sequentially exposed to changing preferences.</figcaption></figure><p>Learning from interaction naturally enables continual personalization. In our experiments, models adapt to user-specific preferences from few interactions and can unlearn previously expressed preferences as user behavior changes. When preferences are complex or complementary, the model adapts continuously without catastrophic forgetting.</p><p>
    </p><strong><h3>Robust and Interpretable Learning</strong></h3><div class="column-list sdpo-model-scale"><div style="width:43.75%" class="column"><p>The learning signal induced by SDPO is local and intuitive. When a user requests a revision or provides relevant follow-up feedback, this is reflected in large token-level self-distillation advantages on wrong tokens. When a user's next message is irrelevant or nonsensical, advantages are near zero and the policy is not meaningfully updated. The model is able to understand in-context whether a user's follow-up is relevant to its message and, if yes, how it could have improved its response.</p><p>
    </p></div><div style="width:56.25%;white-space:initial" class="column"><figure class="image sdpo-model-scale-figure" style="text-align:left;margin-top:-2em;"><a href="figures/user_interactions/advantages.png"><img style="width:2370px;display:block;margin-left:0;margin-right:auto;" src="figures/user_interactions/advantages.png"/></a></figure>
    <!-- </p></div></div><h3><strong>TBD</strong></h3><div class="column-list"><div style="width:43.75%" class="column"><p>...</p><p>
    </p></div><div style="width:56.25%" class="column"><figure class="image" style="margin-top:0;"><a href="figures/SDPO/credit_assignment.png"><img style="width:2370px" src="figures/SDPO/credit_assignment.png"/></a></figure><p> -->
    </p></div></div>
    <h3><strong>Bibtex</strong></h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre class="code code-wrap"><code class="language-Plain Text" style="white-space:pre;word-break:normal">@article{buening2026aligning,
    title = {Aligning Language Models from User Interactions},
    author = {Kleine Buening, Thomas and Hübotter, Jonas and Pásztor, Barna and Shenfeld, Idan and Ramponi, Giorgia and Krause, Andreas},
    year = {2026},
    journal = {arXiv preprint arXiv:XXX},
}</code></pre><p>
    </p></div></article><div class="page-body sans"><p class="nav-link-wrap"><span class="nav-links"><a class="button-link-plain" href="index.html"><span class="button-label">Self-Distillation</span></a><a class="button-link" href="SDFT.html"><span class="button-label">Self-Distillation Enables Continual Learning (SDFT)</span></a></span></p></div><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>
