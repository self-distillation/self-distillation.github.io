<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>SDFT: Self-Distillation Enables Continual Learning</title><link rel="stylesheet" href="common.css"/><style>
    /* SDFT-specific overrides (currently none). */
    </style></head><body class="paper sdft"><article class="page sans"><header><h1 class="page-title">Self-Distillation Enables Continual Learning</h1><p class="page-description"></p></header><div class="page-body"><p><a href="https://idanshen.github.io/">Idan Shenfeld</a><sup>1</sup>,  <a href="https://damanimehul.github.io/">Mehul Damani</a><sup>1</sup>,  <a href="https://jonhue.github.io/">Jonas Hübotter</a><sup>2</sup>, <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a><sup>1</sup></p><p><sup>1</sup>MIT <sup>2</sup>ETH Zurich</p><p><strong>Paper: </strong><a href="https://arxiv.org/abs/2601.19897">https://arxiv.org/abs/2601.19897</a></p><p><strong>Code: </strong><a href="https://github.com/idanshen/Self-Distillation">https://github.com/idanshen/Self-Distillation</a></p><p>
    </p><figure class="image" style="text-align:center;margin-bottom:3.5em;"><a href="figures/SDFT/diagram.png"><img style="width:100%;max-width:800px;" src="figures/SDFT/diagram.png"/></a></figure><h3><strong>Abstract </strong></h3><p>Continual learning, enabling models to acquire new skills and knowledge without degrading existing capabilities, remains a fundamental challenge for foundation models. While on-policy reinforcement learning can reduce forgetting, it requires explicit reward functions that are often unavailable. Learning from expert demonstrations, the primary alternative, is dominated by supervised fine-tuning (SFT), which is inherently off-policy. <mark class="highlight-blue"><strong>We introduce Self-Distillation Fine-Tuning (SDFT), a simple method that enables on-policy learning directly from demonstrations.</strong></mark> SDFT leverages in-context learning by using a demonstration-conditioned model as its own teacher, generating on-policy training signals that preserve prior capabilities while acquiring new skills. Across skill learning and knowledge acquisition tasks, SDFT consistently outperforms SFT, achieving higher new-task accuracy while substantially reducing catastrophic forgetting. In sequential learning experiments, SDFT enables a single model to accumulate multiple skills over time without performance regression, establishing on-policy distillation as a practical path to continual learning from demonstrations.</mark></em></p><figure class="image"><a href="figures/SDFT/teaser_fig.png"><img style="width:2560px" src="figures/SDFT/teaser_fig.png"/></a></figure><p>
    </p><h3><strong>Learning without forgetting</strong></h3><figure class="image"><a href="figures/SDFT/forgetting.png"><img style="width:3254px" src="figures/SDFT/forgetting.png"/></a></figure><p>In a challenging continual learning experiment, we train a single model sequentially on three different tasks. <mark class="highlight-blue"><strong>SDFT successfully learns each new task while retaining performance on previous ones.</strong></mark> In contrast, SFT exhibits severe interference—performance on earlier skills rapidly degrades once training shifts to a new task. This demonstrates that SDFT supports true continual learning, allowing incremental skill acquisition without catastrophic forgetting.</p><p>
    </p><h3><strong>On-policy Learning Leads to Better Generalization</strong></h3><div class="column-list"><div style="width:43.75%" class="column"><p>Beyond reducing catastrophic forgetting, we discovered that SDFT actually learns new tasks better than SFT. Across multiple skill-learning and knowledge-acquisition tasks, <mark class="highlight-blue"><strong>models trained with SDFT consistently achieve higher performance (in-distribution generalization) than those trained with supervised fine-tuning.</strong></mark> Why? Off-policy learning trains only on expert-induced trajectories—but at test time, small errors can push the model into states it never saw during training, causing compounding mistakes. SDFT avoids this mismatch by training on the model's own generated trajectories, teaching it to recover from its own errors rather than just memorizing expert paths. </p><p>
    </p></div><div style="width:56.25%" class="column"><figure class="image"><a href="figures/SDFT/generalization.png"><img style="width:2370px" src="figures/SDFT/generalization.png"/></a></figure><p>
    </p></div></div><h3><strong>SDFT Benefits from Model Scale</strong></h3><div class="column-list"><div style="width:43.75%" class="column"><p>The stronger a model's in-context learning, the better SDFT works. At 3B parameters, the model's ICL ability is too weak to provide meaningful teacher guidance, and SDFT underperforms SFT. But as we scale up, the gains grow consistently: the 7B model achieves a 4-point improvement over SFT, and the 14B model widens the gap to 7 points. <mark class="highlight-blue"><strong> This trend suggests that with frontier models and their strong in-context reasoning, SDFT's advantages will only become more pronounced. </strong></mark> </p><p>
    </p></div><div style="width:56.25%" class="column"><figure class="image" style="margin-top:0;"><a href="figures/SDFT/scaling.png"><img style="width:2370px" src="figures/SDFT/scaling.png"/></a></figure><p>
    </p></div></div><h3><strong>Bibtex</strong></h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre class="code code-wrap"><code class="language-Plain Text" style="white-space:pre-wrap;word-break:break-all">@misc{shenfeld2026selfdistillationenablescontinuallearning,
        title={Self-Distillation Enables Continual Learning},
        author={Idan Shenfeld and Mehul Damani and Jonas Hübotter and Pulkit Agrawal},
        year={2026},
        eprint={2601.19897},
        archivePrefix={arXiv},
        primaryClass={cs.LG},
        url={https://arxiv.org/abs/2601.19897},
  }</code></pre><p>
    </p></div></article><div class="page-body sans"><p class="nav-link-wrap"><span class="nav-links"><a class="button-link-plain" href="index.html"><span class="button-label">Self-Distillation</span></a><a class="button-link" href="SDPO.html"><span class="button-label">Reinforcement Learning via Self-Distillation (SDPO)</span></a></span></p></div><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>
